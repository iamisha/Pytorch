{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NfcpnluzLzo"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ae7dunqczT8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"About the Program\n",
        "What is the course fee for Finance & Investment Bootcamp (FIB 2025)?\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 899/month.\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 5 months. So the total course fee becomes 899*5 = Rs 4500 (approx.)\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "Personal Finance Management\n",
        "Stock Market Fundamentals\n",
        "Technical & Fundamental Analysis\n",
        "Mutual Funds & SIPs\n",
        "Cryptocurrency Basics\n",
        "Risk Management\n",
        "Portfolio Diversification\n",
        "Financial Planning Tools\n",
        "You can check the detailed syllabus here - https://learn.finmentor.in/courses/Finance-Investment-Bootcamp-2025\n",
        "Will we cover taxation or audit topics in this program?\n",
        "No, this program does not include taxation or auditing. It focuses on investments and financial literacy.\n",
        "What if I miss a live session? Will I get a recording of the session?\n",
        "Yes, all sessions are recorded and made available to you in the student dashboard.\n",
        "Where can I find the class schedule?\n",
        "You can view the course timetable here - https://docs.google.com/spreadsheets/d/finance-timetable-sheet/edit?usp=sharing.\n",
        "What is the time duration of each live session?\n",
        "Each live session is approximately 90 minutes long.\n",
        "What language will be used in the sessions?\n",
        "The sessions will be conducted in English and Hindi (bilingual).\n",
        "How will I be informed about upcoming classes?\n",
        "We will send you an email before every scheduled session once you enroll.\n",
        "Can I join the course without prior finance background?\n",
        "Yes, this course is suitable for beginners and enthusiasts with no formal finance education.\n",
        "I am late, can I join the program in the middle?\n",
        "Yes, enrollment is open throughout the year, and you can access all previous content.\n",
        "If I join/pay in the middle, will I be able to see previous content?\n",
        "Yes, once you subscribe, you get full access to all earlier sessions and resources.\n",
        "Do I need to submit assignments?\n",
        "No mandatory submissions. Self-assessment tasks and solutions are provided for your practice.\n",
        "Will the course include real-world case studies?\n",
        "Yes, we analyze real financial markets and scenarios as part of the curriculum.\n",
        "Where can we contact you?\n",
        "Email us anytime at support@finmentor.in\n",
        "Payment/Registration related questions\n",
        "Where do I make payments?\n",
        "All payments should be made on our official website: https://learn.finmentor.in/\n",
        "Can I pay the full course fee at once?\n",
        "No, the course runs on a monthly subscription model.\n",
        "What is the subscription validity?\n",
        "Each payment gives you 30 days of access. For example, payment on 12th June gives access until 12th July.\n",
        "What is the refund policy?\n",
        "We offer a 7-day full refund from the day you register.\n",
        "I'm not in India and can’t pay on the website. What should I do?\n",
        "Please write to us at support@finmentor.in, and we’ll assist with alternative payment methods.\n",
        "Post-registration queries\n",
        "How long do I retain access to the videos?\n",
        "You can access videos as long as your subscription is active. Once full payment is made, access continues until Jan 2026.\n",
        "Why is lifetime access not available?\n",
        "To ensure the course remains affordable and content remains updated.\n",
        "How do I clear doubts after class?\n",
        "Submit your queries through the dashboard form and our team will arrange a 1-on-1 call.\n",
        "Can I ask questions from previous weeks?\n",
        "Yes, simply mention the week/topic when filling out the doubt form.\n",
        "Certificate and Career Support\n",
        "What are the criteria to get a certificate?\n",
        "You must:\n",
        "Complete the full payment of Rs 4500\n",
        "Attempt all quizzes and submit the final portfolio project\n",
        "How do I pay for missed months if I join late?\n",
        "You’ll see the previous month payment options once you pay for the current one.\n",
        "Is placement or job support included?\n",
        "Yes, we provide **career support**, not job guarantees. This includes:\n",
        "Investment firm portfolio building workshops\n",
        "Financial analyst resume guidance\n",
        "Interview readiness sessions\n",
        "Access to industry mentors and webinars\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "fLwm0Y_MzVuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "MMU_RwfbzXt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "tokens = word_tokenize(document.lower())"
      ],
      "metadata": {
        "id": "t28bgAcszaHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocab\n",
        "vocab = {'<unk>':0}\n",
        "\n",
        "for token in Counter(tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token] = len(vocab)\n",
        "\n",
        "vocab"
      ],
      "metadata": {
        "id": "G30GxEjgzcfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "SOOEZ94P0dQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = document.split('\\n')"
      ],
      "metadata": {
        "id": "RefNavJe1Cva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(sentence, vocab):\n",
        "\n",
        "  numerical_sentence = []\n",
        "\n",
        "  for token in sentence:\n",
        "    if token in vocab:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab['<unk>'])\n",
        "\n",
        "  return numerical_sentence\n"
      ],
      "metadata": {
        "id": "x52A3E1K1zjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_numerical_sentences = []\n",
        "\n",
        "for sentence in input_sentences:\n",
        "  input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))\n"
      ],
      "metadata": {
        "id": "eu66Zo3e1Wh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_numerical_sentences)"
      ],
      "metadata": {
        "id": "XxJesAQC1et3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = []\n",
        "for sentence in input_numerical_sentences:\n",
        "\n",
        "  for i in range(1, len(sentence)):\n",
        "    training_sequence.append(sentence[:i+1])"
      ],
      "metadata": {
        "id": "80rIx4aq6ele"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sequence)"
      ],
      "metadata": {
        "id": "V_aGJ0fy7swk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence[:5]"
      ],
      "metadata": {
        "id": "wrFzZ4DD8Anu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "\n",
        "for sequence in training_sequence:\n",
        "  len_list.append(len(sequence))\n",
        "\n",
        "max(len_list)"
      ],
      "metadata": {
        "id": "q2Z_fiVZ8GRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence[0]"
      ],
      "metadata": {
        "id": "4bIcIRd088EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = []\n",
        "for sequence in training_sequence:\n",
        "\n",
        "  padded_training_sequence.append([0]*(max(len_list) - len(sequence)) + sequence)"
      ],
      "metadata": {
        "id": "dtPg5uRN9Cc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(padded_training_sequence[10])"
      ],
      "metadata": {
        "id": "hqZssF989X-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)"
      ],
      "metadata": {
        "id": "0_wVpepb9iE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence"
      ],
      "metadata": {
        "id": "ogKvdXa79yxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_training_sequence[:, :-1]\n",
        "y = padded_training_sequence[:,-1]"
      ],
      "metadata": {
        "id": "Tz8fwCok90m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "1Ed_PLHJ-Dgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "eReVrcX9-EUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "fR059hVd-IAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X,y)"
      ],
      "metadata": {
        "id": "KLX0clQM_j9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "uYHaeSuI_nJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "7ZUeD3l6_oZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, 100)\n",
        "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
        "    self.fc = nn.Linear(150, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    intermediate_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
        "    output = self.fc(final_hidden_state.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "0TEukXmWDEn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(len(vocab))"
      ],
      "metadata": {
        "id": "YcQEVc9aVgr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Lvm7W6L1X6P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "oXwq43NRYD3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "1faORN1VYFdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch_x, batch_y in dataloader:\n",
        "\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(batch_x)\n",
        "\n",
        "    loss = criterion(output, batch_y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "LRLc1cbrYVVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def prediction(model, vocab, text):\n",
        "    # Get model's device (cpu or cuda)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized_text = word_tokenize(text.lower())\n",
        "\n",
        "    # Text -> numerical indices\n",
        "    numerical_text = text_to_indices(tokenized_text, vocab)\n",
        "\n",
        "    # Padding\n",
        "    padded_text = torch.tensor([0] * (61 - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Move input to same device as model\n",
        "    padded_text = padded_text.to(device)\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(padded_text)\n",
        "\n",
        "    # Get predicted index\n",
        "    value, index = torch.max(output, dim=1)\n",
        "\n",
        "    # Convert index back to word\n",
        "    predicted_word = list(vocab.keys())[list(vocab.values()).index(index.item())]\n",
        "\n",
        "    # Merge with text\n",
        "    return text + \" \" + predicted_word\n"
      ],
      "metadata": {
        "id": "O9f6DkX-ZM-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(model, vocab, \"\")"
      ],
      "metadata": {
        "id": "VsRgcJysbGCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_tokens = 20\n",
        "input_text = \"You can access videos\"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "  output_text = prediction(model, vocab, input_text)\n",
        "  print(output_text)\n",
        "  input_text = output_text\n",
        "  time.sleep(0.5)\n"
      ],
      "metadata": {
        "id": "t_JPACfEbNPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader1 = DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "JXsV4AnNXNnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients\n",
        "        for batch_x, batch_y in dataloader1:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(batch_x)\n",
        "\n",
        "            # Get the predicted word indices\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Compare with actual labels\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = calculate_accuracy(model, dataloader, device)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Py7o0rJJc5pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bQnBuShXG5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}